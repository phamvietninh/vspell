\documentclass[a4paper,oneside]{book} % -*- coding: vietnamese-utf8 mode: latex -*-

\usepackage[hmargin={2cm,2cm}]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\renewcommand{\rmdefault}{utm}
\renewcommand{\sfdefault}{uhv}
\renewcommand{\ttdefault}{ucr}
\usepackage[utf8]{vietnam}
\newcommand{\note}[1]{\underline{#1}}
\newcommand{\comment}[1]{}

\newtheorem{algo}{Thuật toán}

\title{Chương trình bắt lỗi chính tả tiếng Việt}
\author{Nguyễn Thái Ngọc Duy\\0012020}

\begin{document}
\maketitle{}
\tableofcontents{}
\listoffigures{}


\chapter{Giới thiệu}
\label{cha:intro}
\begin{center}
I kept the right ones out

And let the wrong ones in

Had an angel of mercy

To see me through all my sins

There were times in my life

When I was goin' insane

Tryin' to walk through the pain

****

And when I lost my grip

And I hit the floor

Yeah, I thought I could leave

But couldn't get out the door

I was so sick n' tired

Of livin' a lie

I was wishing that I would die

****

It's amazing

With the blink of an eye

You finally see the light

It's amazing

That when the moment arrives

You know you'll be alright

It's amazing

And I'm saying a prayer

To the desperate hearts tonight


****


That one last shot's a Permanent Vacation

And a how high can you fly with broken wings

Life's a journey - not a destination

And I just can't tell just what tomorrow brings


****


You have to learn to crawl

Before you learn to walk

But I just couldn't listen

To all that righteous talk

I was out on the street

Just tryin' to survive

Scratchin' to stay alive


****


``To all of you people out there

Wherever you are - remember:

The light at the end of the tunnel

May be you - goodnight''
\end{center}


\chapter{Thông tin nền}
\label{cha:background}


\section{Lý thuyết về ngôn ngữ tự nhiên}
\label{sec:natural-language}

\section{Lý thuyết về tiếng Việt}
\label{sec:Vietnamese}



\section{Tổng quan về lỗi chính tả}

\note{Lỗi chính tả là gì?}

Theo \cite{LoiChinhTa} thì:
\begin{verse}
  Chữ viết là hệ thống ký hiệu bằng đường nét đặt ra để ghi tiếng nói
  và có những qui tắc, qui định riêng. Muốn viết đúng chính tả tiếng
  Việt, ta phải tuân theo những qui định, qui tắc đã được xác lập.

  Chính tả là cách viết chữ được xem là chuẩn, tức là viết đúng âm
  đầu, đúng vần, đúng dấu (thanh), đúng quy định về viết hoa, viết
  tắt, viết thuật ngữ.
\end{verse}

\note{nổ thêm tí ở đây}

\note{Phân loại lỗi, nguyên nhân gây lỗi}

\subsection{Phân loại lỗi}

Theo \cite{LoiChinhTa}, lỗi bao gồm các loại sau:
\begin{itemize}
\item Lỗi thanh điệu. Chủ yếu là lỗi nhầm lẫn hai thanh hỏi, ngã.
\item Lỗi về âm đầu. Thường lẫn lộn các âm đầu sau: C/K, G/Gh, Ng/Ngh,
  Ch/Tr, S/X, V/D/Gi/R, W/Hw/Ngw/Qu.
\item Lỗi về âm chính. Thường lẫn lộn các âm chính sau: ai/ay/ây,
  ao/au/âu, ăm/âm, ăp/âp, iu/iêu/êu, im/iêm/em, ip/iêp/êp/ep,
  oi/ôi/ơi, om/ôm/ơm, op/ôp/ơp, ong/ông, oc/ôc, ui/uôi, um/uôm, up/(uôp),
  ưi/ươi, ưu/ươu, ưm/ươm, (ưp)/ươp.
\item Lỗi về âm cuối. Thường lẫn lộn chữ ghi các âm cuối trong các vần
  sau:
  an/ang, at/ac, ăn/ăng, ăt/ăc, ân/âng, ât/âc, en/eng, et/ec, ên/ênh,
  êt/êch, in/inh, it/ich, iên/iêng, iêt/iêc, ơn/(ơng), ơt/(ơc),
  un/ung, ut/uc, uôn/uông, uôt/uôc, ưn/ưng, ưt/ưc, ươn/ương, ươt/ươc.
\item Sai quy cách viết hoa
\end{itemize}

\noindent Theo \cite{Tuoi}, lỗi được phân làm hai loại:
\begin{itemize}
\item Từ bị sai không nằm trong từ điển. (do sai dấu hỏi---ngã, bỏ dấu
  không đúng vị trí, gõ nhầm ký tự trên bàn phím, gõ thiếu ký tự, gõ
  dư ký tự, gõ sai do gõ theo cách phát âm từng vùng, gõ nhầm vị trí
  các ký tự trong từ \ldots)
\item Từ bị sai nhưng vẫn có trong từ điển. (sai cách phát âm, nhầm ký
  tự, bỏ dấu sai, thiếu/thừa ký tự\ldots)
\end{itemize}

\noindent Theo \cite{Chang} thì lỗi bao gồm:
\begin{itemize}
\item Giống phiên âm
\item Giống hình dạng chữ viết
\item Giống nghĩa
\item Giống cách gõ
\end{itemize}

\section{Xử lý lỗi chính tả}
%Nêu tổng quan, nói cụ thể hơn ở phần sau ``previous work''

Với \cite{Tuoi} thì việc xử lý lỗi bao gồm:
\begin{itemize}
\item Từ không có trong từ điển thì bắt lỗi ở cấp từ vựng.
\item Từ có trong từ điển thị bắt lỗi ở cấp cú pháp.
\end{itemize}

%% Với \cite{Chang}, giải pháp gồm:
%% \begin{itemize}
%% \item Composite confusing character substitution
%% \item Advanced word class bigram language. LM score is composed base
%%   language model score and substitution penalty score.
%% \end{itemize}

%% qua các bước:
%% \begin{enumerate}
%% \item Input and Clause Segmentation
%% \item Composite Confusing Character Substitution
%% \item Language Model Evaluation
%% \item Comparision and Correction
%% \item Output
%% \end{enumerate}

Việc tìm lỗi dựa chủ yếu trên từ điển. So sánh từng từ trong câu với từ điển, những từ
không có trong từ điển là những từ có khả năng bị lỗi. Sau đó dựa trên
một số thuật giải hoặc các độ đo để tìm ra từ gần đúng với từ đó, chọn làm
từ đề nghị.

Việc tự động sửa lỗi chủ yếu dựa trên một tập các từ hay bị lỗi (then-than,
there-their \ldots), sử dụng ngữ cảnh xung quanh để xác định từ đúng
hay sai.

Đối với ngôn ngữ đơn lập như tiếng Việt, vấn đề mới phát sinh là không
thể xác định rõ ràng ranh giới từ. Với tiếng Anh và các ngôn ngữ biến
cách khác, khoảng trắng thường được dùng để phân cách hai từ. Trong tiếng Việt,
khoảng trắng được dùng để phân cách hai tiếng. Ngoài ra, việc định
nghĩa từ trong tiếng Việt vẫn chưa thống nhất\cite{worddef}. Luận văn
này sử dụng ``từ'' như là ``từ từ điển''.  

Để bắt lỗi chính tả trong tiếng Việt, có thể dựa trên nhiều cách khác
nhau. Cách đầu tiên đơn giản là tìm ranh giới từ cho tiếng Việt, sau
đó chuyển về bài toán bắt lỗi chính tả của tiếng Anh, dựa trên từ
điển. Cách khác là bắt lỗi mà không cần tách từ\cite{Tuoi}
Cách nữa dựa trên ý tưởng ``phân tích cú pháp câu, nếu ta không thể
phân tích cú pháp của câu, nghĩa là câu đó sai chính tả'' \cite{iccc}.

Luận văn này làm theo hướng tách từ, sau đó tìm lỗi chính tả dựa trên
từ điển.

\section{Những nghiên cứu trước đây}

Điều hiển nhiên dễ thấy là nếu câu bị sai chính tả thì ta không thể
tách từ đúng được. Vấn đề ở đây là phải tách từ trong câu bị sai chính
tả. Do đó không thể áp dụng trực tiếp các thuật toán tách từ thông
thường. 

\cite{Oflazer} khi xử lý hình thái trong tiếng Thổ Nhĩ Kỳ gặp trường
hợp khá giống với trường hợp này. Tác giả phải tách hình thái từ
trong điều kiện từ đó bị sai chính tả. Do đặc tính
ngôn ngữ chắp dính\footnote{agglunative language}, số tiếp vĩ ngữ
nhiều, liên tiếp nhau, gây khó khăn cho việc nhận dạng tiếp vĩ ngữ,
cũng như không thể phân biệt những tiếng nào hợp thành một từ trong
một chuỗi tiếng trong tiếng Việt. Tác giả dùng một hàm độ đo, tạo ra
các tiếp đầu ngữ có khả năng thay thế dựa trên độ đo này, sau đó sử
dụng WFST\footnote{Weighted Finite State Transducer} để tìm chuỗi tiếp
vĩ ngữ thích hợp nhất.

Bài toán nhận dạng tiếng nói trong tiếng Anh cũng gặp trường hợp tương tự
(\cite{Ravishankar}). Sau công đoạn xử lý âm thanh, ta nhận được
một chuỗi các âm tiết (phoneme). Ta phải chuyển nhóm âm tiết này thành
chuỗi từ. Do âm thanh thường bị nhiễu, nên các âm tiết có thể
không chính xác hoàn toàn. Ngoài ra, do đặc tính của tiếng Anh nên
cùng một chuỗi âm tiết có thể suy ra nhiều chuỗi từ khác nhau. Tác giả
sử dụng lưới từ để tạo ra các chuỗi từ có khả năng từ chuỗi âm tiết,
sau đó n-gram trên từ để lượng giá các chuỗi từ.

Theo \cite{Chang}, có thể dùng nhiều Language Model khác nhau như
character bigram, word bigram, inter-word character bigram (IWCB), POS
bigram, word class bigram. Các bước thực hiện:
\begin{enumerate}
\item Tách đoạn. Dùng cách ký tự như `?' `!' `,'
  `:' `.' \ldots{} để tách.
\item Thay thế các ký tự nhập nhằng. 
\item Lượng giá Mô hình Ngôn ngữ. Tác giả sử dụng một phiên bản của
  IWCB và SA-class bigram. IWCB là, những từ cùng ký tự đầu tiên được
  nhóm thành 1 class, những từ cùng ký tự cuối cùng được nhóm thành
  class. Bigram dựa trên hai nhóm này. ``Modified'' là tính luôn word
  freq. SA-class bigram dùng SA algorithm để nhóm.
\item So sánh và sửa chữa.
\item Kết xuất.
\end{enumerate}

Dựa trên những nghiên cứu này, có thể thấy giải pháp cho việc tách từ khi bị
sai chính tả, là phát sinh một loạt các từ có khả năng thay thế, với
giả định trong tập từ này sẽ có từ đúng chính tả, thay thế từ sai chính
tả ban đầu. Sau đó sử dụng tách từ tìm một cách tách tốt nhất. Sau khi
tìm được cách tách từ, ta có thể tra từ điển để tìm xem từ nào bị sai.


\subsection{Tách từ}

\note{Liệt kê các phương pháp tách từ}

Bài toán tách từ cho ngôn ngữ đơn lập đã được đặt ra từ lâu, chủ yếu
để giải quyết cho tiếng Hoa, tiếng Nhật. Các thuật toán tách từ có thể
được phân loại như sau:

\begin{description}
\item[Dựa theo luật] 
  \begin{itemize}
  \item Longest Matching, Greedy Matching Models (Yuen Poowarawan, 1986;
    Sampan Rarurom, 1991)
  \item Maximal Matching Models. This model is divided into ``forward
    maximum match'' and ``backward maximum match'', for which the fully
    comprehensive dictionary is indispensible. However, it is obvious
    that there is no comprehensive dictionary and depending on different
    contexts that this model requires suitable dictionaries.
    \begin{itemize}
    \item Thai, Sornlertlamvanich (1993).
    \item Chinese, Chih-Hao Tsai (1996), MMSeg 2000.
    \end{itemize}
  \end{itemize}
\item[Dùng thống kê]
  This approach on the word context in considering
  the information of the neighboring words to issue revelant
  decisions. There are two points to be resolved for this approach,
  which are the context width and applied statistical method. As far
  as the context is concerned the wider the more complex.

  As far as the context is concerned, the first-order HMM has always
  been applied. However, this method greatly depends on the corpus for
  its training. In case one method is applicable for the political
  corpus, it can not be applied to literal ones. In addition, there
  are some words of high probability but of syntactical function only,
  which lessens the role of probability.
  \begin{itemize}
  \item HMM, based on Viterbi algorithm. (Asanee Kawtraku, 1995;
    Surapant,1995).
  \item EM. Xianping, 1996.
  \end{itemize}
\item[Các cách khác]
  Most of these approaches are hybrid in combination with such other
  linguistic models as WFST, TBL (Julia, 1996). However, due to the
  requirement of various manipulation, the processing becomes quite
  time-consuming but it accuracy proves to be high.

  However, the linguistic knowledge, which is applied in the
  rule-based models, is rarely found in all the above models.
\end{description}

\cite{Sproat} sử dụng WFST để tách từ, huấn luyện bằng EM dựa trên
cách tách từ đúng nhất.

\cite{wordseg} sử dụng giải pháp tương tự như \cite{Sproat} nhưng cải
tiến bằng cách cáp dụng Neural Networks để giải quyết nhập nhằng dựa
vào POS. Nghiên cứu đề cập đến một số vấn đề tiền xử lý tiếng Việt như
xác định tên riêng, từ láy, phân tích hình thái.

\cite{Ravishankar} đề nghị tạo ra lưới từ\footnote{word lattice} sau
đó sử dụng thuật toán tìm đường đi ngắn nhất để tìm cách tách từ tốt
nhất dựa trên 2-gram hoặc 3-gram.

\cite{LAH} sử dụng n-gram và quy hoạch động để tách từ,
không dùng từ điển. Giải pháp này tương tự như \cite{softcount}, tuy
nhiên chỉ tính xác suất cách tách từ tốt nhất thay vì tính tổng xác
suất mọi cách tách từ như trong \cite{softcount}.

\cite{Chunyu} kết hợp ngram, lập trình quy hoạch động để tách từ. Xài
soft-count thay vì ``hard-count'' như \cite{Chang}. \cite{Chunyu} còn
đề nghị dùng case-based learning.

\subsection{Huấn luyện tách từ}

\note{Tại sao không xài bộ tách từ đã có?} Hai vấn đề:
\begin{itemize}
\item Chất lượng của bộ tách từ hiện có. (2-gram tốt hơn unigram
  \ldots...)
\item Nếu thống kê trên mọi cách tách từ thì không thể dùng bộ tách từ
  hiện có vì chỉ tạo ra kết quả sau cùng.
\end{itemize}


Có thể huấn luyện dựa trên dữ liệu mẫu, hoặc dữ liệu thô. Do hầu hết
các phương pháp tách từ đều dựa trên ngram ($n\ge 1$) nên rất cần có
khối lượng dữ liệu huấn luyện lớn nhằm đạt tính bao quát. Dữ liệu
mẫu, được tách từ sẵn, thường không đủ để huấn luyện. Giải pháp chủ
yếu là huấn luyện dựa trên dữ liệu thô, chưa tách từ.

Thuật toán thường dùng nhất để huấn luyện trên dữ liệu thô là thuật
toán EM. Nhiều người đã cố gắng cải tiến EM theo nhiều cách khác nhau
nhằm nâng cao chất lượng huấn luyện, đồng thời hạn chế những khuyết
điểm của EM. 

\cite{self-supervised} đề nghị cách giải quyết hạn chế ``tối ưu cục
bộ'' của EM, bằng cách phân phối lại từ vựng sau mỗi lần chạy và khởi
động lại EM với điểm khởi đầu tốt hơn.

\cite{text-tiling} sử dụng một lượng nhỏ ngữ liệu huấn luyện có chất
lượng cao (\textit{seed set}) và một lượng lớn ngữ liệu có chất lượng
không thật bảo đảm (\textit{training set}), xử lý qua 4 bước:
\begin{enumerate}
\item \textbf{Phân đoạn tập huấn luyện}
\item \textbf{Phân hạng tập huấn luyện}
\item \textbf{Tổ hợp tập huấn luyện}
\item \textbf{Language model pruning}
\end{enumerate}

\chapter{Cài đặt}
\label{cha:implementation}

Chương trình gồm hai phần: phần bắt lỗi chính tả và phần huấn luyện.

\section{Trình bắt lỗi chính tả}


\subsection{Quy trình chung}
\label{sec:spellcheck}

Việc bắt lỗi chính tả của một văn bản được xử lý lần lượt qua những
bước sau:
\begin{enumerate}
\item \textbf{Tiền xử lý} Tách văn bản thành những đoạn ngắn. Tách
  đoạn thành từng tiếng. Đánh dấu các ký hiệu, dấu ngắt dòng, các số,
  tên riêng \ldots
\item \textbf{Bắt lỗi tiếng} Kiểm tra các tiếng với các tiếng đã có
  trong từ điển. Báo lỗi những tiếng không có trong từ điển. Sau đó 
  \textbf{Đưa ra giải pháp thay thế}.
\item \textbf{Tạo lưới từ} Tìm ra mọi từ có thể có trong câu. Xem
  giải thích về lưới từ bên dưới. Lồng trong phần này là phần
  \textbf{Phát sinh từ thay thế}
\item \textbf{Tách từ} Dựa vào lưới từ, đưa ra cách tách từ tốt nhất.
\item \textbf{Bắt lỗi từ} Dựa vào từ điển và cách tách từ đã có,
  tìm những từ nào không có trong từ điển. Những từ này được xem là từ
  sai. Sau đó \textbf{Đưa ra giải pháp thay thế}
\end{enumerate}

\subsection{Tiền xử lý}
\label{sub:preprocess}

Dựa vào flex để tách thành các token. Mục đích là để tách các dấu câu
ra khỏi tiếng. Ví dụ \fbox{"chào"} sẽ được tách thành 3 token
\fbox{"} \fbox{chào} \fbox{"}. Quy tắc được dùng như sau:

\begin{verbatim}

SENTENCE_FINAL                  [.?!]
HYPHEN                          [\-]
OPEN_SINGLE_QUOTE               [\`]
CLOSE_SINGLE_QUOTE              [\']
RIGHT_PAREN                     [\"\)\]\}\>\']


LETTERS_AND_NUMBERS             [a-zA-Z0-9]
LETTERS_NUMBER_AND_THEN_SOME    [a-zA-Z0-9]
APOSTROPHE                      \'

SINGLE_CHARACTER                [a-zA-Z0-9]

WHITE_SPACE                     [ \t\n]
NEWLINE                         [\n]
INVISIBLE                       [^\040-\176]


{SENTENCE_FINAL}+{RIGHT_PAREN}*                                 |
{HYPHEN}+                                                       |
{OPEN_SINGLE_QUOTE}+                                            |
{CLOSE_SINGLE_QUOTE}+                                           |

{LETTERS_NUMBER_AND_THEN_SOME}+{LETTERS_AND_NUMBERS}            |
{LETTERS_AND_NUMBERS}+{APOSTROPHE}                              |

{SINGLE_CHARACTER}                   { xuất từ }

({WHITE_SPACE}|{INVISIBLE}|{NEWLINE})+                { bỏ qua }

\end{verbatim}


\subsubsection{Tách câu}

Dựa vào các dấu câu để ngắt câu ra thành từng đoạn để xử lý. Mỗi
đoạn sẽ được xử lý độc lập với nhau. Đoạn ở đây có thể là một câu,
nhưng cũng có thể là một 
phần của câu. Luận văn này sẽ dùng từ ``câu'' để ám chỉ ``đoạn''. Nếu
các thông tin ở mức cao hơn được sử dụng (như thông tin cú pháp, ngữ
nghĩa \ldots) thì phải thật sự xử lý trên câu chứ không phải trên 
đoạn. Các đoạn được phân cách bởi các dấu câu \fbox{.} \fbox{,}
\fbox{;} \fbox{(} \fbox{)} \ldots 

\subsubsection{Chuẩn hoá}

Do trong tiếng Việt có nhiều từ có thể viết theo các
cách khác nhau về vị trí dấu thanh điệu, ví dụ, ``hoà'' và
``hòa''. Bởi vậy cần phải chuẩn hoá sao cho chương trình xem ``hòa''
và ``hoà'' là một. Giải pháp đưa ra là tách dấu thanh điệu ra khỏi
từ, biểu diễn dấu thanh điệu bằng ký tự đầu tiên trong từ. Như vậy,
``hoà'' và ``hòa'' đều có cùng cách biểu diễn là ``2hoa'', trong khi
đó ``hoa'' được biểu diễn là ``0hoa'', ``hồng'' được biểu diễn là
``2hông''.

\subsubsection{Chữ viết hoa}

Xử lý chữ viết hoa. Chữ viết hoa dùng để biểu diễn tên riêng, từ viết
tắt hoặc dùng cho chữ đứng đầu câu. Do đó cần phân biệt chữ đầu câu có
phải là chữ bắt đầu tên riêng hay không. Ngoài ra, cần xác định tên riêng khi
tìm được chữ viết hoa bắt đầu tên riêng. Các văn bản tiếng Việt chưa
hoàn toàn thống nhất về quy tắc viết hoa. Ví dụ, có tài liệu dùng
``Cộng hoà Xã hội Chủ nghĩa Việt Nam'', nhưng có tài liệu lại dùng
``Cộng Hoà Xã Hội Chủ Nghĩa Việt Nam''.

Do văn bản đầu vào có khả năng bị sai chính tả nên không thể xác định
ngay chính xác có phải là cụm từ viết hoa đúng hay không. Bước này cố
gắng phát hiện ra những cụm từ viết hoa và thêm vào lưới từ. Do vậy,
phần này được thực hiện bên trong phần \textbf{Phát sinh từ thay thế}. 

Có thể dùng thuật giải như sau: Nếu bắt đầu bằng chữ viết hoa, và chữ
đầu được viết thường cùng những từ kế tiếp của nó là một từ trong từ
điển, thì xem như đó là một từ. Nếu một chuỗi từ viết hoa liên tục
(hai từ trở lên) thì xem như đó là một từ (một tên riêng), và được
đánh dấu từ lạ $UNK$.

Với tên riêng của người Việt. Một danh sách các tên tiếng Việt được
thu thập nhằm rút ra những chữ thường dùng khi đặt tên. Những chuỗi
các chữ hoa liên tiếp được hình thành từ những chữ trong danh sách này
sẽ được xem như là tên riêng, đánh mã $PROP$.


%% \begin{algo} Xử lý chữ hoa

%% \label{algo:propername}
%%   \begin{enumerate}
%%   \item Lấy 1 chữ. Kiểm tra ký tự đầu tiên xem có phải ký tự hoa hay
%%     không.
%%   \item Nếu đây là chữ đầu câu, phát sinh bình thường dựa theo bản
%%     viết thường của chữ này. Ngoài ra tiếp tục thực hiện các bước
%%     dưới.
%%   \item Duyệt từ chữ đó trở đi, lấy chuỗi dài nhất có thể dựa trên
%%     $sid$. Phát sinh từ mới dựa trên từ lấy được. Đặt xác suất 0.80.
%%   \item Nếu không tìm được từ nào, duyệt lại như bước trên, dùng
%%     $scid$.
%%   \item Nếu chữ kế tiếp cũng là chữ hoa, duyệt từ đó trở đi để lấy
%%     chuỗi chữ hoa dài nhất ($scid$). Phát sinh từ mới
%%     mã $PROP$. 
%%   \end{enumerate}
%% \end{algo}

\subsubsection{Từ láy UNIMPL}

Phần này được thực hiện bên trong phần \textbf{Phát sinh từ thay thế}.

\begin{itemize}
\item Láy toàn bộ (Vd. ba ba, chuồn chuồn \ldots)
\item Láy âm đầu (Vd. chặt chẽ, trong trẻo \ldots)
\item Láy điệp vần (Vd. lông bông, lã chã \ldots)
\item Láy điệp âm, láy vần (Vd. thủ thỉ, tủm tỉm \ldots)
\item Hai thanh của tiếng thường ở cùng một hệ \ldots
\end{itemize}

\note{Hổng thèm làm :-)} Vấn đề từ láy cũng tương tự như  tên riêng. Giải
pháp có thể áp dụng là dựa vào các quy tắc tạo từ láy để phát hiện
những từ có khả năng là từ láy và thêm vào lưới từ. 

Đối với các từ láy trên 2 tiếng. Do số lượng các từ láy dạng này không
nhiều nên có thể thêm các từ láy này vào từ điển.
%% Nhưng những ``từ láy''
%% ``nhưng những'' có thể giết chết module tách từ một cách êm dịu. Chắc
%% nên áp dụng giải pháp ``80 từ láy, 20 từ thường''. Không biết được hay
%% không, phiêu quá \ldots

\begin{algo} Xử lý từ láy

\label{algo:poem}
  \begin{enumerate}
  \item Lấy 2 chữ liên tiếp.
  \item Kiểm tra quy tắt từ láy.
  \item Nếu đúng, phát sinh thêm từ láy mới, mã $POEM$.
  \end{enumerate}
\end{algo}

%% Có lẽ nên lọc ra danh sách từ láy bằng tay, đánh nhãn toàn bộ là
%% $POEM$, hoặc dùng nhãn riêng. Heuristic này không đáng tin lắm.
%% Cách hình thành từ láy của con người như thế nào ?? Từ láy thường gắn
%% liền với hình tượng, cảm giác gì đó. Làm sao người ta có thể cảm nhận
%% được một từ có phải là từ láy hay không??

\subsubsection{Từ nước ngoài, từ viết tắt, các ký hiệu, \ldots}

\note{Coi như từ sai chính tả}
\note{không phân biệt từ nước ngoài và từ sai chính tả}
Xử lý tiếng nước ngoài, các ký hiệu chuyên ngành, các từ viết tắt. Do
trình bắt lỗi không có kiến thức về các lĩnh vực chuyên 
ngành, cũng như các thứ tiếng trên thế giới, nên việc áp dụng tri thức
để phân loại là điều hết sức khó khăn. Giải pháp được dùng ở đây là
coi tất cả những từ nước ngoài, từ viết tắt, ký hiệu \ldots đều là sai
chính tả, trừ khi được người dùng được vào danh sách từ nước ngoài, từ
viết tắt. Các con số được đánh dấu riêng bằng mã $NUM$.
``Số'' ở đây được coi là bất cứ chữ nào bắt đầu bằng số. Ví dụ, ``0lit'',
``0.2'', ``0-4'' \ldots{} đều được coi là số. 

\subsection{Phát sinh từ thay thế}

Phần này được sử dụng trong lúc tạo lưới từ và kiểm tra chính
tả. Mục đích là, cho trước một từ, phát sinh những từ ``gần giống''
với từ đó. Việc định nghĩa như thế nào là ``giống'' ở đây dựa theo các
nguyên nhân gây ra lỗi chính tả:
\begin{itemize}
\item Lỗi phát âm. Ví dụ, ``vu'' và ``du''. 
\item Do lỗi bàn phím. Có thể do gõ nhầm những phím lân cận. Ví dụ: gõ
  ``tôi'' thành ``tôu''. Khắc phục lỗi này dựa vào bố trí phím trên
  bàn phím. Độ đo là khoảng cách từ phím tạo ra ký tự trong từ cho
  trước và những từ chung quanh. Thông thường lỗi này chỉ xảy ra một
  lần trong mỗi từ. Ngoài ra còn lỗi liên quan đến cách kiểu gõ tiếng
  Việt.
\item Lỗi OCR\footnote{Optical Character Recognition}. Lỗi dạng này
  phụ thuộc nhiều vào phông chữ được dùng trên văn bản giấy. Tuy nhiên
  vẫn có một số lỗi chung như ``a'' và ``c'', ``d'' và ``cl'' \ldots
\item Lỗi sai về mặt từ vựng, cú pháp. Lỗi này hiện thời không xử lý
  do cần nhiều tri thức về ngôn ngữ tiếng Việt.
\item Lỗi không rõ nguyên nhân. Với dạng lỗi này, ta dùng hàm độ đo,
  tính số lần thêm/xóa/thay đổi/hoán vị mỗi ký tự giữa hai từ. Hàm độ đo
  được dùng được nêu trong \cite{Oflazer}, sẽ được trình bày lại bên
  dưới.
\end{itemize}

%% \note{Hmm..} có lẽ nên gắng thêm vào các từ phát sinh một ``hệ số chính
%% xác''. Từ gốc là từ có hệ số chính xác cao hơn so với các từ được phát
%% sinh. Good or bad? Hmmm... Làm sao để làm? Hmmm Hmmm

\subsubsection{Lỗi phát âm}

Lỗi phát âm phụ thuộc vào cách phát âm của từng
vùng. \cite{LoiChinhTa} liệt kê các trường hợp lỗi thông dụng
nhất. Những quy tắc này được áp dụng để, ví dụ, từ   ``du'' ta tạo ra
``vu''. 

Để phát sinh từ dựa trên lỗi phát âm, cần phân tích cấu trúc của từng
tiếng. Một tiếng bao gồm âm đầu, vần và thanh. Vần gồm âm đệm, âm
chính và âm cuối. Trong các thành phần của tiếng, âm chính là bắt buộc
phải có. Các thành phần còn lại có thể không có. Ta có thể biểu diễn
cấu trúc âm tiếng theo sơ đồ trạng thái như hình \ref{fig:syllable}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{syllable}
  \caption{Sơ đồ trạng thái phân tích cấu trúc tiếng}
  \label{fig:syllable}
\end{figure}

Khi phân tích tiếng, có thể có một số nhập nhằng giữa các thành phần
của tiếng. Vd, ``lúa'' bao gồm âm chính ``ua'' hay âm đệm ``u'' và âm
chính ``a'' \ldots

Ta có thể phân tích tiếng dùng FST. Tuy nhiên, qua hình
\ref{fig:syllable} có thể thấy chỉ có tám cách để hình thành
tiếng. Cài đặt theo tám cách này đơn giản và hiệu quả hơn dùng FST
tổng quát. Việc sắp xếp thứ tự áp dụng từng cách và một số heuristic
sẽ giúp loại bỏ nhập nhằng khi phân tích tiếng. 

\subsubsection{Lỗi bàn phím UNIMPL}

Chương trình giả định sơ đồ bố trí của bàn phím EN-US được dùng. Do
thông thường chỉ gặp một lỗi gõ nhầm với phím ngay bên cạnh này mỗi
từ, nên chương trình chỉ lưu danh  sách những phím lân cận với từng
phím, dựa trên bàn phím EN-US. Ví dụ: $A \rightarrow (S,Q,W,X,Z)$. 

Với những phím có hai ký tự như phím `2' (2 và @) thì @ sẽ được thêm
vào tập các phím lân cận với 2 và ngược lại.

Do với mỗi phím có khoảng 8 phím lân cận. Một chữ dài trung bình 8 ký
tự sẽ phát sinh ra một tập $8^8$ các chuỗi có khả năng. Trong số này
chỉ có một số rất ít là chữ thật sự, đúng quy tắc chính tả. Tuy nhiên
việc xử lý một khối lượng lớn như vậy là không thể. Vì vậy chương
trình giả định chỉ nhập gõ sai tối đa hai phím với mỗi chữ, nhằm giảm
thiểu bùng nổ tổ hợp.

Với các kiểu gõ như VNI, TELEX. Chương trình cố gắng ``phục hồi'' từ
những chữ gõ sai nếu phát hiện được. Vd, ``nguyê4n'' sẽ tạo ra
``nguyễn''. Ta cài đặt bộ gõ VNI, TELEX, sau đó đưa chuỗi các ký tự
của chữ đang xét qua bộ gõ phục hồi các dấu. Bước này được thực hiện
sau bước trên, để nếu gõ nhầm phím dấu kế bên thì vẫn có thể phục hồi
lại.

\subsubsection{Lỗi không rõ nguyên nhân UNIMPL}

\label{algo:ed}
Lỗi này dựa vào {\em độ đo khoảng cách hiệu chỉnh} được đề cập
trong \cite{Oflazer}. Các thao tác hiệu chỉnh được đo gồm {\em chèn,
xóa, thay thế một ký tự} hoặc {\em hoán vị hai ký tự kề nhau}, để có
thể chuyển đổi từ này thành từ kia. Đặt $X = x_1, x_2, \ldots, x_m$ và
$Y = y_1,y_2,\ldots,y_n$ là hai chuỗi có độ dài tương ứng là $m$ và
$n$. $X[i]\quad(Y[j])$ biểu diễn chuỗi con ban đầu của X (Y) từ đầu từ
đến ký tự thứ $j$. Cho $X$ và $Y$, độ đo $ed(X[m],Y[n])$ được tính như
sau:
\begin{equation}
\begin{array}{rll}
  ed(X[i+1],Y[j+1]) &= ed(X[i],Y[j]) & \text{nếu $x_{i+1}=y_{j+1}$
  (ký tự cuối như nhau)}\\
                    &= 1+min\{ed(X[i-1],Y[j-1]), & \text{nếu $x_i=y_{j+1}$}\\
                    &\qquad\qquad ed(X[i+1],Y[j]), & \text{và $x_{i+1}=y_j$}\\
                    &\qquad\qquad ed(X[i],Y[j+1])\}\\
                    &= 1+min\{ed(X[i],Y[j]),&\text{trường hợp khác}\\
                    &\qquad\qquad ed(X[i+1],Y[j]),\\
                    &\qquad\qquad ed(X[i],Y[j+1])\}\\
  ed(X[0],Y[j])     &=j & 0 \le j \le n\\
  ed(X[i],Y[j])     &=i & 0 \le i \le n\\\\
  ed(X[-1],Y[j])    &=ed(X[i],Y[-1]) = max(m,n)&\text{Biên}
\end{array}
\end{equation}

Thuật toán có thể được khử đệ quy khi cài đặt bằng quy hoạch
động. Nhận thấy $ed(X[i+1],Y[i+1])$ cần dùng
$ed(X[i],Y[i])$,$ed(X[i-1],Y[i-1])$,$ed(X[i],Y[i+1])$ hoặc
$ed(X[i+1],Y[i])$, ta có thể tính từ lần lượt từ thấp đến cao theo
đường zig-zag bắt đầu từ đỉnh (1,1), sau đó đến đường chéo phụ (1,2),
(2,1), rồi đường chéo (1,3),(2,2),(3,1) \ldots

\begin{algo} Tính hàm ed()

  \begin{enumerate}
  \item Đặt (0,0) là 0 nếu x[0] = y[0] hoặc 1 nếu ngược lại.
  \item Đặt (0,j) là j
  \item Đặt (i,0) là i
  \item (i,j)=(i-1,j-1) nếu x[i]=y[j]
  \item (i,j)=1+min\{(i-2,j-2),(i,j-1),(i-1,j)\} nếu x[i] = y[j-1]
    hoặc x[i-1]=y[j]
  \item (i,j)=1+min\{(i-1,j-1),(i,j-1),(i-1,j)\} trong các trường hợp còn lại
  \end{enumerate}
\end{algo}

Ví dụ: tính độ sai khác giữa chữ ``thắng'' và ``tắgn''. Ta tính các
giá trị như bảng \ref{tab:ed-example}. Vậy có hai khác biệt giữa hai
chữ. Khác biệt thứ nhất là xoá ký tự `h'. Khác biệt thứ hai là hoán vị
hay ký tự `n' và `g'.

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    X&-&-&t&ắ&g&n\\\hline
    -&0&0&0&0&0&0\\\hline
    -&0&0&1&2&3&4\\\hline
    t&0&1&\bf 0&\bf 1&\bf 2&\bf 3\\\hline
    h&0&2&\bf 1&\bf 1&\bf 2&\bf 3\\\hline
    ắ&0&3&\bf 2&\bf 1&\bf 1&\bf 2\\\hline
    n&0&4&\bf 3&\bf 2&\bf 2&\bf 1\\\hline
    g&0&5&\bf 4&\bf 3&\bf 2&\bf 2\\\hline
  \end{tabular}
  \caption{Bảng $ed$ của ``thắng'' và ``tắgn''}
  \label{tab:ed-example}
\end{table}

\subsection{Tạo lưới từ}
\label{sub:lattice}
Lưới từ\footnote{word lattice} là một đồ thị có hướng không chu trình,
với các nút là các từ trong câu, cạnh là đường nối giữa hai từ kề
nhau, hướng thể hiện hướng của câu (từ trái sang phải). Lưới từ
chứa tất cả 
các từ có khả năng xuất hiện trong câu. Các từ được liên kết với nhau
theo trật tự trong câu. Khi duyệt từ nút gốc đến nút đích, ta sẽ được
một cách tách từ cho câu. Xem hình \ref{fig:wordlattice}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{wordlattice}
  \caption{Lưới từ của câu ``Học sinh học sinh học''}
  \label{fig:wordlattice}
\end{figure}

Khi tạo lưới từ trong chương trình bắt lỗi chính tả, thuật toán không
chỉ phát sinh những từ được tạo từ đoạn, mà còn những từ {\em có thể
có} được phát sinh từ đoạn. Xem hình \ref{fig:wordlattice1}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{wordlattice1}
  \caption{Lưới từ mở rộng của câu ``Học sinh học sinh học''}
  \label{fig:wordlattice1}
\end{figure}

Lưới từ được tạo bằng thuật toán Viterbi. Mỗi tiếng trong câu được
duyệt qua để tìm ra tất cả các từ có thể có trong đoạn. Sau đó tập hợp
các từ này lại. 

\begin{algo} Tạo lưới từ (cơ bản)

Cho câu S có n tiếng. State là đỉnh, còn ``nút i'' là cạnh. Ta duyệt
lần lượt qua các cạnh để tìm  ra các từ. Duyệt i từ 1 đến n:
\begin{itemize}
\item Tạo state gốc cho nút i.
\item Xét các state, nếu tiến thêm được một bước thì lưu lại state mới (i+1).
\item Nếu không tiến được thì xóa state.
\item Nếu hoàn tất một từ thì lưu lại.
\end{itemize}
\end{algo}

Sau khi hoàn tất, đánh dấu tất cả các chữ xuất hiện trong các từ tìm
được. Do từ điển không hoàn chỉnh, và do các ký hiệu, từ viết tắt
\ldots{}, sẽ có một số chữ không nằm trong bất kỳ từ nào tìm được. Do
đó ta cần duyệt lại, và đánh dấu những chữ này là từ ``UNK''. Việc tạo
thêm các từ $UNK$ sẽ đảm bảo tạo ra một đồ thị liên thông. Nếu đồ
thị không liên thông, ta không thể áp dụng thuật toán tìm đường đi
ngắn nhất.

Ngoài lưới từ cơ bản, ta có thể tạo lưới 2-từ. Lưới 2-từ
tương tự như lưới từ, tuy nhiên mỗi nút là một cặp 2 từ đi liền nhau
trong câu. Thuật toán tạo lưới 2-từ được nêu trong \cite{Ravishankar},
được tóm tắt lại như sau (xem hình \ref{fig:wordlattice2}):

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{wordlattice2}
  \caption{Lưới 2-từ của câu ``Học sinh học sinh học''}
  \label{fig:wordlattice2}
\end{figure}

\begin{algo}Tạo lưới n-từ từ lưới (n-1)-từ

\begin{enumerate}
\item Nếu nút ($w$) có $n$ từ đứng liền trước nó ($w_i$),
  $i=1,2,\ldots,n$ trong lưới từ gốc, nó sẽ được lặp lại $n$ lần trong
  lưới từ mới, tên là ($w_{i}w$), tương ứng với $i=1,2,\ldots,n$.
\item Nếu ($w_i$) nối với ($w_j$) trong lưới từ gốc, nối tất cả
  ($w_xw_i$) với ($w_iw_j$) $x$ bất kỳ.
\item Giá trị của ($w_iw_j$) là giá trị của cạnh ($w_i$) ($w_j$) trong
  lưới từ cũ.
\item Giá trị của cạnh ($w_iw_j$) ($w_jw_k$) là 3-gram của $w_i$, $w_j$
  và $w_k$.
\end{enumerate}
\end{algo}

%% Các lưới n-từ có đặc điểm là tăng nhanh số nút và số cạnh, nhưng số
%% tầng vẫn không đổi (Đồ thị càng ngày càng ``mập'', nhưng ``cao''
%% không nổi ;-) )


\subsubsection{Lưu lưới từ}

Các nút trong lưới từ được lưu tập trung vào một mảng
(WordEntries). Truy cập nút được thực hiện thông qua index của mảng
này. Mỗi nút chứa các thông tin về từ của nút đó, bao gồm vị trí từ,
số tiếng, index trong WordEntries\ldots{}

Words là một mảng tương ứng với vị trí từng tiếng trong câu. Vị trí
của mỗi tiếng sẽ được liên kết với một danh sách các từ bắt đầu tại
tiếng đó. Ta có thể coi danh sách này như danh sách các cạnh nối từ
nút đang xét đến các nút kế tiếp.


\subsection{Tách từ}
\label{sub:wordseg}

Dùng thuật toán tìm kiếm theo độ ưu tiên PFS\footnote{Priority-First
Search---PFS} để tìm đường đi ngắn nhất trên đồ thị. Khoảng cách giữa hai
điểm trong đồ thị là xác suất 2-gram của hai từ. Bài toán tìm đường
đi ngắn nhất, có thể áp dụng PFS, Prime, Djisktra. PFS được chọn vì
lưới từ, qua khảo sát, có thể được coi là một đồ thị thưa.

Để áp dụng 3-gram thay vì 2-gram, ta sẽ sử dụng lưới 2-từ. Sau đó áp
dụng thuật toán PFS cho đồ thị mới.

Cách làm này không thể thực hiện với n-gram ($n > 3$) vì khi đó số
nút/cạnh trong đồ thị sẽ tăng đáng kể. Ngoài ra, PFS cũng không áp
dụng được trong trường hợp sử dụng những độ đo như đo số lỗi sai đã
xảy ra \ldots Trong trường hợp đó ta nên sử dụng thuật toán A*\cite{Ravishankar}

%% XXXX. Nếu WFST cộng viterbi, beam pruning \ldots thì sao nhỉ?


\subsection{Tìm từ thay thế}

Đối với tiếng Anh, bước này dùng thuật toán như metaphone, Soundex
\cite{soundex}, Phonetex \cite{phonetex}. Ý tưởng 
là dựa vào heuristic, thay thế các nhóm ký tự trong từ thành một ký
hiệu tương trưng cho một âm. Sau đó so sánh độ hiệu chỉnh giữa âm của
từ sai và các âm của từ trong từ điển. Do tiếng Việt đọc sao ghi vậy,
nên bước chuyển từ từ sang âm có thể bỏ qua. Bước còn lại là áp dụng
thuật toán đo độ hiệu chỉnh giữa hai từ như đã nêu trong \ref{algo:ed}
Ta có thể so sánh từng ký tự, hoặc so sánh theo âm (ví dụ, ``th'' thay
vì ``t'' và ``h'').

Chương trình sẽ liệt kê XXXX (chọn theo tiêu chí nào?, 10 từ lớn nhất
hay sao đây)

XXXX. thuật toán tính ed.

\section{Giao tiếp với người dùng}
\label{sec:ui}

Cố gắng khai thác tri thức của người sử dụng, nhằm bổ trợ cho trình
bắt lỗi, giảm thiểu các sai sót do trình bắt lỗi gây ra.

Hỏi người dùng càng nhiều càng tốt. Luôn luôn có một tùy chọn ``không
biết'' cho người dùng.

\begin{itemize}
\item Phần bắt lỗi tiếng.
\item Nhận dạng từ sai.
\item Nhận dạng ký hiệu, tên riêng nước ngoài.
\end{itemize}

Sau mỗi lần sữa lỗi, cần chạy lại thuật toán để loại tác động của lỗi
sai.

Văn bản xử lý được phân thành nhiều tầng:
\begin{enumerate}
\item Văn bản thô toàn bộ. Chuỗi các ký tự.
\item Văn bản thô (câu).
\item Dạng token. Tách ra từng chuỗi con.
\item Dạng sentence. Một số token sẽ được gán id. Một số khác bị bỏ
  qua. 
\item Dạng lattice. Đồ thị các id (có thể giống hoặc khác với id ở cấp
  sentence) và các liên kết. 
\item Dạng segmentation. chuỗi id nhất định.
\end{enumerate}

Hiện tại, việc thay đổi thông tin ở tầng trên sẽ hủy mọi thông tin ở
tầng bên dưới.

Lưu lại các thông tin của người dùng như thế nào? Các thông tin này là
``global'' hay ``local''?

\section{Trình huấn luyện}
\label{sec:training}

\subsection{Tiền xử lý}
Giống như phần \ref{sub:preprocess}


\subsection{Tạo Lưới từ}
Giống như phần \ref{sub:lattice}, nhưng chỉ xét những từ nào thực sự
có trong câu, sử dụng lưới từ thay vì lưới từ mở rộng.


\subsection{Thống kê n-gram}
\label{sub:wordcount}

Nếu dùng PFS để tách từ, ta chỉ có 1 cách tách từ tốt nhất. Việc đếm
từ sẽ dựa trên cách tách từ này.

Như đã nói, có thể dùng WFST hoặc dùng thuật toán trong
\cite{softcount} (tạm gọi là thuật toán ``soft-count'') để tách
từ. WFST phải dùng kèm với beam pruning để tránh bùng nổ số tổ hợp các
cách tách từ. Sau khi dùng WFST, ta còn n cách tách từ, có thể đếm
fractional count (được đề cập bên dưới) trên các cách tách từ này.

Trường hợp nhiều cách tách từ, ta có thểm đếm từ trên tất cả các cách
tách, thay vì chỉ đếm trên cách tách từ tốt nhất. Cách này phản ánh
tầm ảnh hưởng của các từ tốt hơn. Ví dụ, ta có 3 cách tách từ với xác
suất các cách tách từ tương ứng lần lượt là 0.5, 0.4 và 0.1. Cách chỉ
dùng cách tách từ tốt nhất sẽ chỉ tính những từ trong cách tách từ
đầu, với giá trị mỗi từ là 1.0. Cách tính này ``dồn phiếu'' của 2 cách
sau cho cách đầu. Trường hợp sau, các từ trong cách 2 và 3 vẫn được
tính. Số đếm của mỗi từ không còn là 1, mà là xác suất của cách tách
từ chứa từ đó. Trở lại ví dụ, các từ trong cách một sẽ được cộng thêm
0.5 thay vì 1. Ngoài ra các từ trong cách 2 và 3 lần lượt được cộng
0.4 và 0.1. Dễ thấy, các từ trong cách tách từ thấp sẽ không tăng số
đếm đáng kể, do đó không thể gây ảnh hưởng lớn đến quyết định tách từ.
Cách cộng dồn số như vậy được gọi là ``fractional count'' (hay trong
\cite{softcount} gọi là ``soft-count'').

Luận văn này thuật toán soft-count để đếm mọi cách tách từ. Soft-count
thực tế không kết xuất ra một cách tách từ cụ thể nào (và do vậy nên
cũng không thể áp dụng để tìm cách tách từ tốt nhất được!). Thay vào
đó, thuật toán đếm mọi từ thể có. Thuật toán được mô tả trong
\cite{softcount} không dùng từ điển. Mọi chuỗi con trong câu đều được
cho là từ. Điểm này, vừa là điểm mạnh vì không cần dùng từ
điển, nhưng cũng là điểm yếu vì yếu tố này làm giảm độ chính xác một
cách đáng kể (khoảng 20\verb#%#, như kết quả trong \cite{softcount}).

Giả sử câu S có $n$ tiếng, $2^{n-1}$ cách tách từ khác nhau, xác suất
mỗi cách tách từ là $p(1),p(2),\ldots,p(n)$.
Với mỗi từ trong một cách tách từ, ta cộng thêm một khoảng
$\displaystyle\frac{p(i)}{\sum_{i=1}^n{p(i)}}$ cho từ đó. Soft-count dùng
lập trình quy hoạch động để thực hiện quá trình này.

Thuật toán được dùng ở đây là thuật toán soft-count, được hiệu chỉnh
sử dụng từ điển và lưới từ để hạn chế những từ không phải là từ.

Cho lưới từ của câu S. Gọi $L(W)$ là tập những từ nối đến nút
$W$. Tương tự, $R(W)$ là tập những từ được nối đến từ nút $W$.
Với mỗi nút $W$ trong S, tổng xác suất các cách tách từ có chứa nút
$W$ là:
$$P(W)=P^{left}(W)p(W)P^{right}(W)$$
Trong đó:
\begin{itemize}
\item $P^{left}(W)$ là tổng xác suất các cách tách từ tính từ đầu câu
  đến $W$.
\item $P^{right}(W)$ là tổng xác suất các cách tách từ tính từ $W$ đến
  hết câu.
\end{itemize}

$$
P^{left}(W) = \left\{
    \begin{array}{ll}
      p(W)&\text{nếu W là nút head}\\
      \displaystyle\sum_{W' \in L(W)}p(W')P^{left}(W')&\text{ngược lại}\\
    \end{array}
  \right.
$$

Tương tự

$$
P^{right}(W) = \left\{
    \begin{array}{ll}
      p(W)&\text{nếu W là nút tail}\\
      \displaystyle\sum_{W' \in R(W)}p(W')P^{right}(W')&\text{ngược lại}\\
    \end{array}
  \right.
$$

\begin{algo}Tính $P^{left}$

\begin{enumerate}
\item Đặt $P^{left}(head) = p(head)$
\item Đặt $P^{left}(W) = 0$ với mọi $W$ còn lại.
\item Duyệt lần lượt các nút $W$ theo thứ tự từ trái sang phải, tính
  theo vị trí bắt đầu của $W$ trong câu. Với $W' \in R(W)$
  cộng thêm $p(W)P^{left}(W)$ vào $P^{left}(W')$
\end{enumerate}
\end{algo}

Thuật toán tương tự được áp dụng để tính $P^{right}$.

Sau khi tính được $P^{left}$ và $P^{right}$, ta có thể tính fractional
count cho các từ trong câu bằng cách duyệt tất cả các nút trong lưới từ,
cộng thêm vào $\displaystyle\frac{P(W)}{P^{left}(tail)}$ cho từ
$C$. Thực tế, ta sẽ lồng bước này vào trong thuật toán tính
$P^{right}$, vì thuật toán cũng phải duyệt qua tất cả các từ.

\begin{algo}Tính $P^{right}$
  
\begin{enumerate}
\item Đặt $P^{right}(tail) = p(tail)$
\item Đặt $P^{right}(W) = 0$ với mọi nút còn lại.
\item Duyệt tất cả các nút $W$ từ phải sang trái, tính theo vị trí kết
  thúc của $W$ trong câu.
  \begin{enumerate}
  \item Với $W' \in L(W)$, cộng thêm $p(W)P^{right}(W)$ vào
    $P^{right}(W')$
  \item Tính fractional count cho $W$ theo công thức
    $\displaystyle\frac{P^{left}(W)p(W)P^{right}(W)}{P^{left}(tail)}$
  \end{enumerate}
\end{enumerate}
\end{algo}


Tuy nhiên, thuật toán trên (cũng như thuật toán gốc) sử dụng
uni-gram, trong khi trình bắt lỗi lại dùng 2-gram. Để cho phép thuật
toán dùng 2-gram, ta có thể tạo một lưới 2-từ như cách của
\cite{Ravishankar} khi muốn tách từ dựa trên 3-gram. Tuy nhiên, để áp
dụng cách này với 3-gram đòi hỏi phải tạo lưới 3-từ! Số lượng nút
trong lưới 3-từ nhiều hơn nhiều so với lưới từ gốc, làm giảm tính hiệu
quả của thuật toán. 

Thay vì vậy, thuật toán được hiệu chỉnh để áp dụng 2-gram với lưới từ
thông thường. Thay vì dùng giá trị nút để tính, ta dùng giá trị cạnh
để tính. $p(W)$ sẽ được thay bằng $p(W/W')$.

$$P(W)=P^{left}(W)P^{right}(W)$$

$$
P^{left}(W) = \left\{
    \begin{array}{ll}
      p(W)&\text{nếu W là nút head}\\
      \displaystyle\sum_{W' \in L(W)}p(W/W')P^{left}(W')&\text{ngược lại}\\
    \end{array}
  \right.
$$

Tương tự

$$
P^{right}(W) = \left\{
    \begin{array}{ll}
      p(W)&\text{nếu W là nút tail}\\
      \displaystyle\sum_{W' \in R(W)}p(W'/W)P^{right}(W')&\text{ngược lại}\\
    \end{array}
  \right.
$$

$P(W)$ đại diện cho xác suất tất cả các cách tách từ đi qua nút
$W$. Do ta huấn luyện 2-gram, nên cần xác suất $P(W/W')$ chứ không cần
$P(W)$. Nếu $W$ và $W_{+1}$ chỉ có một cạnh, $P(W)$ cũng chính là
$P(W/W_{+1})$. Nếu có nhiều hơn một cạnh, ta cần dùng cách khác để
tính $P(W/W')$. Với thuật toán tính $P^{right}$ hiện có, ta cộng dồn
$p(W/W')P^{right}$ vào cho $P^{right}(W)$. Vậy trong quá trình cộng
dồn ta có thể tính ngay $P(W/W')$ bằng cách nhân giá trị cộng dồn với
$P^{left}(W)$. Thuật toán tính $P^{left}$ và $P^{right}$ sau cùng như
sau:

\begin{algo}Tính $P^{left}$

\begin{enumerate}
\item Đặt $P^{left}(head) = p(head)$
\item Đặt $P^{left}(W) = 0$ với mọi $W$ còn lại.
\item Duyệt lần lượt các nút $W$ theo thứ tự từ trái sang phải, tính
  theo vị trí bắt đầu của $W$ trong câu.
  Với mỗi $W$ tìm được, cộng thêm $p(W'/W)P^{left}(W)$ vào $P^{left}(W')\quad
  W' \in R(W)$
\end{enumerate}
\end{algo}

\begin{algo}Tính $P^{right}$
  
\begin{enumerate}
\item Đặt $P^{right}(tail) = p(tail)$
\item Đặt $P^{right}(W) = 0$ với mọi nút còn lại.
\item Duyệt tất cả các nút $W$ từ phải sang trái, tính theo vị trí kết
  thúc của $W$ trong câu. Với $W' \in L(W)$:
  \begin{enumerate}
  \item Cộng thêm $p(W/W')P^{right}(W)$ vào $P^{right}(W')$
  \item Tính fractional count cho $W'$ theo công thức
    $\displaystyle\frac{P^{left}(W')p(W/W')P^{right}(W)}{P^{left}(tail)}$
  \end{enumerate}
\end{enumerate}
\end{algo}

Với cách tính này, ta có thể dùng lưới từ cùng với 2-gram. Để tính
3-gram, ta dùng lưới 2-từ. 

\subsection{Tính n-gram}

N-gram được tính như thông thường. Sử dụng backoff chuẩn. Thư viện
SRILM được dùng để tính n-gram.



\section{Cấu trúc dữ liệu}


\subsection{Lưu chuỗi}

Để giảm thiểu không gian bộ nhớ, chương trình không thao tác trực tiếp
trên các chuỗi mà lưu tập trung các chuỗi lại, sau đó dùng mã id để
đánh số từng chuỗi một. Các thao tác so sánh sẽ dựa trên id này. Khi
cần thao tác trên chuỗi, ta sẽ thông qua id để lấy chuỗi gốc.

Các chuỗi được lưu trong một associative array và một array. Một dùng
để ánh xạ chuỗi sang id. Một để ánh xạ id sang chuỗi.

Có thể dùng lexical tree để giảm thiểu không gian lưu chuỗi (UNIMPL)

Mỗi chuỗi có thể có nhiều id: id gắn với chuỗi gốc ({\em id}), id gắn
với chuỗi gốc đã chuẩn hoá ({\em sid}), id gắn với chuỗi viết thường
({\em cid}), id  gắn với chuỗi viết thường đã chuẩn hoá ({\em
  scid}). Các id này giúp việc so sánh được dễ dàng hơn. 

Trong hầu hết quá trình hoạt động, {\em sid} sẽ được dùng. Tuy nhiên,
cũng có khi sẽ dùng {\em scid}. Bởi vậy, mỗi chuỗi sẽ có một id khác
là {\em fid}, hoặc bằng {\em sid}, hoặc bằng {\em scid} \ldots{} {\em
  fid} sẽ được dùng trong hầu hết các hoạt động, trừ phần tiền xử lý.

Mỗi từ sẽ chỉ lưu id và fid.

\subsection{Lưu từ điển}

Từ điển được lưu thành một cấu trúc dạng cây. Hiện tại từ điển lưu các
từ gốc đã chuẩn hoá, và các từ viết thường đã chuẩn hoá.

Với bản huấn luyện, lưu thêm id gốc để có thể xuất ra y như cũ ??
Không cần. Wordlist sẽ lưu theo dạng chuẩn hoá. Như vậy không cần lưu
id gốc. Quá trình training cần có bước chuyển tiếp để đọc wordlist
chưa chuẩn hoá, ghi ra wordlist chuẩn hoá.


\section{Dữ liệu huấn luyện}
\label{sec:data-preprocessing}

Dữ liệu gốc bao gồm: danh sách từ và các văn bản tiếng Việt thu thập
từ mạng VnExpress\footnote{http://www.vnexpress.net}.

Nhiệm vụ:
\begin{itemize}
\item Tạo danh sách tiếng, chuẩn hoá danh sách tiếng.
\item Tiền xử lý ngữ liệu.
\item Huấn luyện n-gram, tạo ra dữ liệu n-gram.
\end{itemize}


\subsection{Dữ liệu nguồn}
\label{sec:data-source}

Dữ liệu nguồn bao gồm danh sách từ và ngữ liệu tiếng Việt. Dữ liệu
được lưu dưới dạng mã VISCII. 
\subsubsection{Danh sách từ}

Đây là một tập tin văn bản lưu tất cả các từ được dùng trong tiếng
Việt, mỗi từ một dòng. Các chữ cách bởi một khoảng trắng.


\subsubsection{Ngữ liệu huấn luyện}

Bao gồm một loạt các tập tin văn bản tiếng Việt. Mỗi dòng là một đoạn
văn bản.

\subsection{Danh sách tiếng}
\label{sub:syllable-list}

Danh sách tiếng chứa mọi tiếng thông dụng nhất được sử dụng trong
tiếng Việt. Danh sách này được rút trích từ danh sách từ, sau đó được
lưu ở dạng chuẩn hoá.

Danh sách tiếng được rút trích tự động khi chương trình nạp danh sách
từ.

%Chương trình syllable-extract được dùng để tạo danh sách này. Cách
%dùng:
%\begin{verbatim}
%$syllable-extract < word-list > syllable-list
%\end{verbatim}


\subsection{Tiền xử lý ngữ liệu huấn luyện}
\label{sec:training-data-preprocessing}

Dữ liệu thu được từ VnExpress bao gồm một loạt các file html, chia
thành nhiều thư mục.

htmltidy được sử dụng để chuyển dữ liệu html về dạng xhtml. Các file
html của VnExpress không hoàn toàn tuân theo chuẩn html, bao gồm hai
trường hợp:

\begin{itemize}
\item Sử dụng tag <vne>
\item Không đóng tag <frame>
\end{itemize}

Hai lỗi này được tự động phát hiện và sửa chữa trước khi chuyển dữ
liệu cho htmltidy.

Sau khi chạy htmltidy, ta được dữ liệu xhtml hợp khuôn dạng. Phân tích
các tài liệu này cho thấy phần nội dung chính thường nằm trong tag
<vne>, hoặc trong tag <table> với thuộc tính id là
``CContainer''. XSLT được sử dụng để lọc ra phần nội dung nằm giữa hai
tag này (\verb#z.xslt#).

Trình \verb#convert.sh# sẽ thực hiện bước htmltidy và xslt ở trên. Cách sử
dụng:
\begin{verbatim}
$convert.sh < input  > output
\end{verbatim}

Sau bước này, ta cần lọc bỏ các tag, loại bỏ các entity \ldots{} để
đưa dữ liệu về dạng văn bản thuần túy. Do số lượng tag được định nghĩa
html rất nhiều nên chỉ xử lý những tag nào được sử dụng trong
VnExpress (thông qua chương trình \verb#gettags.pl#). Các tag này được loại
bỏ bằng \verb#html2text.pl#. Các mã utf-8 được sử dụng trong
VnExpress (thông qua chương trình \verb#z.c#) được chuyển về mã VISCII
qua \verb#z.cpp#. Các entity còn lại (lấy bằng \verb#list-entity.pl#)
được xử lý trong \verb#html2text.pl#.

\verb#html2text.pl# sử dụng các tag để tách dữ liệu thành các đoạn. Dữ
liệu sau cùng là các tập tin văn bản, mỗi dòng là một đoạn.

\subsection{Huấn luyện dữ liệu}
\label{sub:training-data}

Chương trình \verb#wfst-train# được dùng để huấn luyện dữ liệu. Thông
tin nhập là danh sách từ, danh sách tiếng, n-gram (nếu có). Thông tin
xuất là n-gram mới.

Chương trình được lặp đi lặp lại nhiều lần.


\chapter{Kết luận}
\label{cha:conclusion}

\section{Hạn chế}
\begin{itemize}
\item Không sử dụng những thông tin cấp cao hơn.
\item Có khả năng nó tự động sửa từ đúng thành từ sai, do từ sai
  ``hay'' được dùng trong câu đó hơn là từ đúng :-P
\end{itemize}

\section{Hướng phát triển}
\label{sec:todo}

Viết lại từ đầu :-D

\begin{itemize}
\item Các hướng cải tiến EM.
\item Tư tưởng cơ bản hiện thời là phát sinh mọi trường hợp tương tự
  với trường hợp đang xét và hy vọng giải pháp đúng cũng nằm trong
  đó. Cách này có hai điểm cần bàn:
  \begin{itemize}
  \item Khi nào nên phát sinh, khi nào không? Nói cách khác, khi nào
    ta ``cảm thấy'' chỗ đó có thể bị sai? Statistics/Genetic approach?
  \item Khi phát sinh, nên đánh giá tầm quan trọng của các cách phát
    sinh  cho hợp lý, không nên đánh đồng như hiện nay. How can i do
    that? 
  \end{itemize}
\item Ngoài cách đang làm, còn hướng nào khác không? Liệu speech
  recognition/OCR có ý kiến nào hay không? Cần tìm hiểu mọi thứ liên
  quan đến ``error tolerance''
\item \verb#Syllable::to_str()# output sai từ ``của''.
\item Từ điển bao gồm hai loại viết hoa ``Việt Nam'' và ``A di đà
  phật''. Nên phát sinh thêm loại ``A Di Đà Phật'' luôn. Làm sao chuẩn
  hoá cả hai càng tốt. Chỉ cần lưu ``a di đà phật'' là đủ.
\item \verb#Syllable::parse()# sẽ gặp vấn đề với chữ hoa.
\end{itemize}


\begin{thebibliography}{}
\bibitem{Ravishankar}Mosur K. Ravishankar. 1996. {\em Efficient Algorithms for
  Speech Recognition.} PhD thesis. %CMU-CS-96-143.ps
\bibitem{Oflazer}Kemal Oflazer. 1996. {\em Error-tolerant Finite State
  Recognition with Applications to Morphological Analysis and Spelling
  Correction.} %oflazer96errortolerant.ps.gz
\bibitem{LAH}Le An Ha. {\em A method for word segmentation in
  Vietnamese.} %Ha-03.pdf
\bibitem{Chang}Chao-Huang Chang. {\em A New Approach for
  Automatic Chinese Spelling Correction.} %a-new-approach-for.ps
\bibitem{Sproat}Richard Sproat, William Gale, Chilin Shih, Nancy
  Chang. {\em A Stochastic Finite-State Word-Segmentation Algorithm for
  Chinese.} ACL Vol 22 N3.%J96-3004.pdf
\bibitem{Chunyu}Chunyu Kit, Zhiming Xu, Jonathan
  J. Webster. {\em Integrating Ngram Model and Case-based Learning For
  Chinese Word Segmentation.}%tsdx.ps.gz
\bibitem{softcount}Xianping Ge, Wanda Pratt,
  Padhraic Smyth. {\em Discovering Chinese Words from Unsegmented
  Text.}%mlwordsigir.ps
\bibitem{text-tiling}Jianfeng Gao, Hai-Feng Wang, Mingjing Li, Kai-Fu
  Lee. {\em A Unified Approach to Statistical Language Modeling for
  Chinese.}%icassp00.pdf
\bibitem{}Nianwen Xue.{\em Chinese Word Segmentation as Character
  Tagging.}%paper2.pdf
\bibitem{self-supervised}Fuchun Peng and Dale Schuurmans. {\em Self-Supervised Chinese
  Word Segmentation.}%IDA01.pdf
\bibitem{phonetex}Victoria J. Hodge, Jim Austin. {\em An Evaluation of
  Phonetic Spell Checkers}%an-evaluation-of-phonetic.ps.gz
\bibitem{soundex}K. Kukich. 1992. {\em Techniques for Automatically Correcting
  Words in Text.}
\bibitem{}Cláudio L. Lucchesi and Tomasz Kowaltowski. {\em Applications of
  Finite Automata Representing Large Vocabularies.}%lucchesi92applications.ps.gz
\bibitem{}Bo-Hyun Yun, Min-Jeung Cho, Hae-Chang Rim. {\em Segmenting Korean
  Compound Nouns using Statistical Information and a Preference
  Rule. }%PACLING97.ps 
\bibitem{}Roger I. W. Spooner and Alistair D. N. Edwards. {\em User
  Modelling for Error Recovery: A Spelling Checker for Dyslexic
  Users}%spooner97user.ps.gz 
\bibitem{}Theppitak Karoonboonyanan, Virach Sornlertlamvanich,
  Surapant Meknavin. {\em A Thai Soundex System for Spelling Correction.}%tsdx.ps.gz
\bibitem{}Justin Zobel and Philip Dart. {\em Finding Approximate Matches in
  Large Lexicons.}%zobel95finding.ps.gz
\bibitem{}Sun Maosong, Shen Dayang, Huang Changning. {\em CSeg\&Tag1.0: A
  Practical Word Segmenter and POS Tagger for Chinese Texts.}%A97-1018.pdf
\bibitem{wordseg}Đinh Điền, Hoàng Kiếm, Nguyễn Văn Toàn. {\em Vietnamese Word
  Segmentation.}%0047-02.pdf
\bibitem{}Bidyut Baran Chaudhuri. {\em Reversed word dictionary and
  phonetically similar word grouping based spell-checker to Bangla
  text.}%bangla.pdf
\bibitem{}Timothy Gambell, Charles D. Yang. {\em Scope and Limits of
  Statistical Learning in Word Segmentation.}%gambell_yang.pdf
\bibitem{iccc}Andi Wu, George Heidorn, Zixin Jiang, Terence
  Peng. {\em Correction of Erroneous Characters in Chinese Sentence
  Analysis.}%ICCC-2001.pdf
\bibitem{}Fuchun Peng, Xiangji Huang, Dale Schuurmans, Shaojun
  Wang. {\em Text Clasification in Asian Languages without Word
  Segmentation.}%IRAL2003.pdf 
\bibitem{}Yalin Wang, Ihsin T. Phillips, Robert
  Haralick. {\em Statistical-based Approach to Word Segmentation.}%wordicpr.pdf
\bibitem{}{\em Combining Syntactical And Statistical Language Constraints
  in Context-dependent Language Models for Interactive Speech
  Applications.}%K026.pdf
\bibitem{LoiChinhTa}TS. Lê Trung Hoa. 2002. {\em Lỗi chính tả và cách khắc phục. NXB Khoa
  học Xã hội.} 
\bibitem{LoiTuVung}PGS. Hồ Lê, TS. Trần Thị Ngọc Lang, Tô Đình Nghĩa. 2002. {\em Lỗi từ
  vựng và cách khắc phục.} NXB Khoa học Xã hội.
\bibitem{Tuoi}PTS. Phan Thị Tươi, KS. Nguyễn Hứa Phùng, KS. Huỳnh Vụ
  Như Liên, KS. Phạm Quyết Thắng. 1998. {\em Bắt lỗi chinh tả tự động cho
  tiếng Việt bằng máy tính.}
\bibitem{worddef}Đinh Điền. 2000. {\em Từ tiếng Việt.} VNU-HCMC.
\end{thebibliography}

\end{document}


Giả sử câu S có $n$ tiếng $c_1,c_2,\ldots,c_n$, có $|S|$ cách tách từ
$S_1,S_2,\ldots,S_{|S|}$ và cách tách từ  $S_i$ được
tách thành $|S_i|$ từ $W_{i_1},W_{i_2},\ldots,W_{i_{|S_i|}}$ với 
$W_i$ là một từ xác định bắt đầu ở tiếng thứ $i$ chứa $|W_i|$ tiếng, và
$i_j$ là vị trí từ thứ $j$ trong cách tách câu $i$.

Ta có:
$$p(i)=\sum_{j=1}^{|S_i|}{p(W_{i_j})}$$

Một câu sẽ được tách thành:
$$P(W_i) = P_{i}^{left}p(W_{i})P_{i+|W_i|}^{right}$$

$P_i^{left}$ là xác suất tất cả các tổ hợp từ có thể có từ 
tiếng thứ nhất đến tiếng thứ $i$.

$P_{i}^{right}$ là xác suất tất cả các tổ hợp từ có thể có từ tiếng
thứ  $i$ đến hết câu. 

Dễ thấy, với mỗi từ $C$ trong câu $S$, fractional count W của từ sẽ là
$\displaystyle\frac{P(W)}{\sum_i^{|S|}p(i)}$.

$\displaystyle\sum_i^{|S|}p(i)$ cũng chính là $P_{n+1}^{left}$ theo
định nghĩa $P_i^{left}$.

Ta sẽ dùng quy hoạch động để tính $P_i^{left}$ và $P_i^{right}$

$$
P_i^{left} = \left\{
  \begin{array}{lr}
    1 & i=1\\
    p(W_i) & i=2\\
    \sum_{j=1}^{i-1}p(c_j\ldots c_{i-1})P_j^{left} & i>2
  \end{array}
\right.
$$

$$
p(c_i\ldots c_j) = \left\{
  \begin{array}{ll}
    p(W_i)&\text{nếu } c_i\ldots c_j \text{ tạo thành } W_i\\
    0&\text{ngược lại}
  \end{array}
\right.
$$

Thuật toán tính $P^{left}$ như sau:
\begin{enumerate}
\item Đặt $P_1^{left} = 1$
\item Đặt $P_i^{left} = 0\quad \forall i \in [2\ldots n+1]$
\item Duyệt i từ 1 đến n, tìm tất cả các từ $W_i$ (do tại có thể có
  nhiều từ bắt đầu tại tiếng $i$).
  Với mỗi từ $W_i$ tìm được, cộng thêm $p(W_i)$ vào $P_{i+|W_i|}^{left}$
\end{enumerate}

Thuật toán tương tự được áp dụng để tính $P^{right}$.

Sau khi tính được $P^{left}$ và $P^{right}$, ta có thể tính fractional
count trong câu bằng cách duyệt tất cả các từ có thể có trong câu,
cộng thêm vào $\displaystyle\frac{P(W)}{P_{n+1}^{left}}$ cho từ
$C$. Thực tế, ta sẽ lồng bước này vào trong thuật toán tính
$P^{right}$, vì thuật toán cũng phải duyệt qua tất cả các từ.

Vậy thuật toán tính $P^{right}$ là:
\begin{enumerate}
\item Đặt $P_{n+1}^{right} = 1$
\item Đặt $P_i^{right} = 0\quad \forall i \in [1\ldots n]$
\item Duyệt i từ n+1 đến 1.
  \begin{enumerate}
  \item tìm tất cả các từ $W_j$ sao cho $j+|W_j|=i$.
    Với mỗi từ $W_j$ tìm được, cộng thêm $p(W_j)$ vào
    $P_j^{right}$
  \item Tính fractional count cho tất cả các từ $W_i$ ($i \le n$)
  \end{enumerate}
\end{enumerate}


Ví dụ: câu ``học sinh học sinh học'' có 8 cách tách từ
\begin{verbatim}
học-sinh học-sinh học
học-sinh học sinh học
học-sinh học sinh-học
học sinh học sinh học
học sinh học sinh-học
học sinh học-sinh học
học sinh-học sinh học
học sinh-học sinh-học
\end{verbatim}
\def\Zhs{\text{học-sinh}}
\def\Zsh{\text{sinh-học}}
\def\Zh{\text{học}}
\def\Zs{\text{sinh}}
Ta có
$$
\begin{array}{rl}
P_1^{left}(\Zhs) &= p(\Zhs_1/\phi)\\
P_1^{left}(\Zh) &= p(\Zh_1/\phi)\\
P_2^{left}(\Zs) &= p(\Zs_2/\Zh_1)P_1^{left}(\Zh)\\
 &=p(\Zs_2/\Zh_1)p(\Zh_1/\phi)\\
P_2^{left}(\Zsh) &= p(\Zsh_2/\Zh_1)P_1^{left}(\Zh)\\
 &=p(\Zsh_2/\Zh_1)p(\Zh_1/\phi)\\
P_3^{left}(\Zhs) &= p(\Zhs_3/\Zhs_1)P_1^{left}(\Zhs)+p(\Zhs_3/\Zs_2)P_2^{left}(\Zs)\\
 &=p(\Zhs_3/\Zhs_1)p(\Zhs_1/\phi)+p(\Zhs_3/\Zs_2)p(\Zs_2/\Zh_1)p(\Zh_1/\phi)\\
 &=p(\Zhs_1,\Zhs_3)+p(\Zh_1,\Zs_2,\Zhs_3)\\
P_3^{left}(\Zh) &= p(\Zh_3/\Zs_2)P_2^{left}(\Zs)+p(\Zh_3/\Zhs_1)P_1^{left}(\Zhs)\\
 &=p(\Zh_3/\Zs_2)p(\Zs_2/\Zh_1)p(\Zh_1/\phi)+p(\Zh_3/\Zhs_1)p(\Zhs_1/\phi)\\
 &=p(\Zh_1,\Zs_2,\Zh_3)+p(\Zhs_1,\Zh_3)\\
\end{array}
$$

$$
\begin{array}{rl}
P_5^{right}(\Zh) &= p(\Phi/\Zh_5)\\
P_4^{right}(\Zsh) &= p(\Phi/\Zsh_4)\\
P_4^{right}(\Zs) &= p(\Zh_5/\Zs_4)P_5^{right}(\Zh)\\
 &=p(\Zh_5/\Zs_4)p(\Phi/\Zh_5)\\
P_3^{right}(\Zh) &= p(\Zsh_4/\Zh_3)P_4^{right}(\Zsh)+p(\Zs_4/\Zh_3)P_4^{right}(\Zs)\\
 &=p(\Zsh_4/\Zh_3)p(\Phi/\Zsh_4)+p(\Zs_4/\Zh_3)(\Zh_5/\Zs_4)p(\Phi/\Zh_5)\\
 &=p(\Zh_3,\Zsh_4)+p(\Zh_3,\Zs_4,\Zh_5)\\
P_3^{right}(\Zhs) &= p(\Zh_5/\Zhs_3)P_5^{right}(\Zh)\\
 &=p(\Zh_5/\Zhs_3)p(\Phi/\Zh_5)\\
 &=p(\Zhs_3,\Zh_5)\\
\end{array}
$$

$$
\begin{array}{rl}
P_3(\Zhs) &= P_3^{left}(\Zhs)P_3^{right}(\Zhs)\\
 &=[p(\Zhs_1,\Zhs_3)+p(\Zh_1,\Zs_2,\Zhs_3)]p(\Zhs_3,\Zh_5)\\
 &=p(\Zhs_1,\Zhs_3)p(\Zhs_3,\Zh_5)+p(\Zh_1,\Zs_2,\Zhs_3)p(\Zhs_3,\Zh_5)\\
 &=p(\Zhs_1,\Zhs_3,\Zh_5)+p(\Zh_1,\Zs_2,\Zhs_3,\Zh_5)
\end{array}
$$


